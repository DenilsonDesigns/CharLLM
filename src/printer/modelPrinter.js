const { oneHotEncode, oneHotDecode } = require("../data/data");
const { forward, W1, W2, trainStep } = require("../model/model");
const { crossEntropyLoss } = require("../utils/loss");

const inputVec = oneHotEncode("h");
const targetVec = oneHotEncode("e");
const result = forward(inputVec);
const loss = crossEntropyLoss(result.outputProb, targetVec);

console.log('\nğŸ§  W1: "', W1);
console.log('\nğŸ§  W2: "', W2);

console.log('\nğŸ§  Forward Pass for "h"');

// console.log("\nğŸ§  hiddenRaw: ", result.hiddenRaw);
// console.log("\nğŸ§  hiddenAct: ", result.hiddenAct);
// console.log("\nğŸ§  outputRaw: ", result.outputRaw);
console.log("\nOutput Probabilities:", result.outputProb);

console.log(
  "\nPredicted Char:",
  oneHotDecode(
    result.outputProb.map((p) => (p === Math.max(...result.outputProb) ? 1 : 0))
  )
);

console.log("\nğŸ”» Cross-Entropy Loss:", loss.toFixed(6)); // 2.07 when predicting "w" instead of "e" (high)

// ğŸ§ª Training loop
console.log("\nğŸ‹ï¸ TRAINING...");

for (let step = 0; step < 100; step++) {
  const loss = trainStep(inputVec, targetVec, 0.1);
  if (step % 10 === 0) {
    console.log(`Step ${step}: Loss = ${loss.toFixed(6)}`);
  }
}

// ğŸ” After training
console.log("\nâœ… AFTER TRAINING");
const resultAfter = forward(inputVec);

console.log("\nOutput Probabilities:", resultAfter.outputProb);
const lossAfter = crossEntropyLoss(resultAfter.outputProb, targetVec);

console.log(
  "Predicted Char:",
  oneHotDecode(
    resultAfter.outputProb.map((p) =>
      p === Math.max(...resultAfter.outputProb) ? 1 : 0
    )
  )
);
console.log("Loss:", lossAfter.toFixed(6));
